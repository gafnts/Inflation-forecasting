---
bibliography: references.bib
---

# Datos y metodología

```{r include=FALSE}
pacman::p_load(tidyverse, kableExtra, here, e1071, stats, tseries, pracma, broom)
main <- read_csv(here("data", "main.csv"))
```

## Análisis de datos

Con el objeto de pronosticar la tasa de variación interanual del índice de precios al consumidor en Guatemala, la especificación de los regresores ha sido propuesta a partir de un procedimiento en el que a 36 series de tiempo---seleccionadas considerando el canal de demanda agregada, el canal de tipo de cambio y las variables del sector externo del mecanismo de transmisión de política monetaria del país [@castillo2014]---les son aplicadas pruebas de causalidad de Wiener-Granger respecto a la variable objetivo para cada uno de los rezagos de mediano plazo y en el que, posteriormente, son filtradas tomando en cuenta (a) la cantidad de retardos que causan en el sentido de Granger al ritmo inflacionario y (b) la magnitud en el nivel de significancia del estadístico $F$ de cada una de las pruebas del *test* previamente mencionado.

El cuadro 1 muestra a las 11 variables cuyos valores pasados son los más útiles, dado el vector inicial de posibles predictores, para pronosticar los valores futuros de la variación en el nivel de precios de la economía. Los datos corresponden al periodo que abarca de enero del 2005 a diciembre del 2021.

\vspace{3mm}

```{=tex}
\begin{center}

\small

\textbf{Cuadro 1:} Variables empleadas durante el proceso de predicción

\normalsize

\end{center}
```
\vspace{-3mm}

```{r include=FALSE}
table_one <- 
  tribble(
  ~ Variable, ~ Descripción, ~ Fuente,
  "ipc", "Tasa de variación interanual del índice de precios al consumidor", "SECMCA",
  "tpm", "Tasa de interés de política monetaria", "SECMCA",
  "m0", "Tasa de variación interanual de la base monetaria restringida", "SECMCA",
  "m1", "Tasa de variación interanual del medio circulante", "SECMCA",
  "imae", "Tasa de variación interanual del índice mensual de actividad económica", "SECMCA",
  "rev", "Tasa de variación interanual de los ingresos totales del gobierno central", "BANGUAT",
  "debt", "Tasa de variación interanual del saldo de la deuda pública externa", "BANGUAT",
  "exports", "Tasa de variación interanual del valor (FOB) de las exportaciones totales", "SECMCA",
  "hydro", "Tasa de variación interanual del valor promedio (dólares por barril) de las importaciones de hidrocarburos", "SECMCA",
  "lendrate", "Tasa de interés activa real en moneda nacional", "SECMCA",
  "itcer", "Tasa de variación interanual del índice de tipo de cambio efectivo real (global)", "SECMCA",
  "cpi", "Tasa de variación interanual del índice de precios al consumidor en Estados Unidos", "FRED"
  )
```

\renewcommand{\arraystretch}{1.5}

```{r table_one, echo=FALSE}
kbl(table_one, 
    booktabs = T, 
    align = c("clc")) %>%
  kable_styling(font_size = 9,
                full_width = T) %>% 
  row_spec(0, 
           bold = TRUE, 
           extra_css = 'vertical-align: middle !important;') %>% 
  column_spec(1, width = "1.7cm") %>% 
  column_spec(2, width = "12cm") %>% 
  group_rows(start_row = 1, 
             end_row = 12, 
             latex_gap_space = "-0.50cm")
```

\vspace{2mm}

\renewcommand{\arraystretch}{1}

```{=tex}
\begin{center}

\small

\textbf{Figura 1:} Variable objetivo y predictores seleccionados

\end{center}
```
\vspace{-11mm}

```{r include=FALSE}
order <- table_one %>% rowid_to_column(var = "id") %>% select(id, "name" = Variable)

fig1 <- 
  main %>% 
  pivot_longer(-fecha) %>% 
  arrange(name, fecha) %>% 
  left_join(order) %>% 
  mutate(name = as_factor(name)) %>% 
  group_by(name)
```

```{r fig_one, echo=FALSE, fig.align = 'center', fig.dim = c(11.5, 4.7)}
fig1 %>% 
  ggplot() +
  geom_line(aes(x = fecha, y = value, color = fct_reorder(name, id))) + 
  scale_y_continuous(limits = c(-99, 99)) +
  theme_bw() +
  theme(
        # X axis
        axis.title.x = element_blank(),
        axis.text.x = element_text(size = 12, color = "black"),
        # Y axis
        axis.title.y = element_blank(),
        axis.text.y = element_text(size = 12, color = "black"),
        # Panel
        panel.grid = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.background = element_rect(fill = "white", color = "black"),
        # Legend
        legend.title = element_blank(),
        legend.text = element_text(size = 11),
        legend.position = "bottom",
        legend.justification = "center",
        legend.margin = margin(.5, 0, 0, 0, "cm")
    ) + 
    guides(colour = guide_legend(nrow = 1))
```

\vspace{2mm}

\normalsize

El cuadro 2 presenta estadísticas descriptivas para cada una de las variables. En ellas se incluyen el primer, tercer y cuarto momentos de sus funciones de densidad de probabilidad, así como la raíz cuadrada del segundo---para evidenciar la volatilidad de las series. En este último sentido, es posible observar que las tasas de cambio interanual del valor promedio de las importaciones de hidrocarburos, las exportaciones totales y los ingresos del gobierno central son los predictores que exhiben la mayor volatilidad durante el periodo del análisis.

Las estadísticas descriptivas también demuestran que esta investigación lidia mayoritariamente con variables que no se distribuyen gaussianamente. En términos de exceso de curtosis, 4 presentan distribuciones sustancialmente leptocúrticas y solamente 2 una distribución aproximadamente normal. Además, el estadístico de asimetría comprueba que 3 series se encuentran ligeramente sesgadas hacia la izquierda, mientras el resto de estas presentan un sesgo positivo (exceptuando a la tasa de variación de los ingresos del gobierno central, cuya asimetría es cercana a 0).

\vspace{2mm}

```{=tex}
\begin{center}

\small

\textbf{Cuadro 2:} Estadísticas descriptivas

\normalsize

\end{center}
```
\vspace{-4mm}

```{r include=FALSE}
main_longer <- 
  main %>% 
  pivot_longer(-fecha, names_to = "Variable", values_to = "value") %>% 
  arrange(Variable, fecha) %>% 
  group_by(Variable)

sumtable <- 
  main_longer %>% 
  summarise(Media = round(mean(value), 2),
            Mediana = round(median(value), 2),
            Mínimo = round(min(value), 2),
            Máximo = round(max(value), 2),
            `Desv. Est.` = round(sd(value), 2),
            Asimetría = round(skewness(value), 2),
            Curtosis = round(kurtosis(value), 2))

obs <- 
  main_longer %>% 
  count(Variable) %>% 
  rename(`Obs.` = n)

table_two <- 
  sumtable %>% 
  left_join(obs) %>% 
  slice(6, 12, 9, 10, 5, 11, 2, 3, 4, 8, 7, 1) %>% 
  as.data.frame()
```

\renewcommand{\arraystretch}{1.3}

```{r table_two, echo=FALSE}
kbl(table_two, 
    booktabs = T,
    align = c("c")) %>%
  kable_styling(font_size = 9,
                full_width = T) %>% 
  row_spec(0, bold = TRUE) %>% 
  column_spec(1, width = "1.5cm") %>% 
  column_spec(6, width = "1.8cm") %>% 
  column_spec(9, width = "1cm") %>% 
  group_rows(start_row = 1, 
             end_row = 12,
             latex_gap_space = "-0.45cm")
```

\renewcommand{\arraystretch}{1}

```{=tex}
\begin{center}

\small

\textbf{Cuadro 3:} Pruebas estadísticas

\normalsize

\end{center}
```
\vspace{-2.7mm}

```{r include=FALSE}
names <- main_longer %>% distinct(Variable) %>% pull()
```

```{r include=FALSE}
autocorrelation <- function(data, variable) {
  data %>% 
    select(!!enquo(variable)) %>% 
    pull() %>% 
    as.ts() %>% 
    Box.test(., lag = 12, type = "Ljung-Box") %>% 
    tidy()
}

list <- list()
autocor <- data.frame()

for (i in names) {
  list <- autocorrelation(main, all_of(i))
  autocor <- rbind(autocor, list)
}

autocor %<>% 
  cbind(main_longer %>% distinct(Variable)) %>% 
  select(Variable, statistic, p.value) %>% 
  mutate(Autocorrelación = paste(round(statistic, 2), "(0.0000)", sep = " ")) %>% 
  select(Variable, Autocorrelación)
```

```{r include=FALSE}
jarque_bera <- function(data, variable) {
  data %>% 
    select(!!enquo(variable)) %>% 
    pull() %>% 
    as.ts() %>% 
    jarque.bera.test(.) %>% 
    tidy()
}

list <- list()
jarque <- data.frame()

for (i in names) {
  list <- jarque_bera(main, all_of(i))
  jarque <- rbind(jarque, list)
}

jarque %<>% 
  cbind(main_longer %>% distinct(Variable)) %>% 
  select(Variable, statistic, p.value) %>% 
  mutate(p.value = round(p.value, 4)) %>% 
  mutate(Normalidad = paste0(round(statistic, 2), " ", 
                             "(", format(p.value, scientific = FALSE), ")")) %>% 
  select(Variable, Normalidad)
```

```{r include=FALSE}
white_test <- function(data, variable) {
  data %>% 
    select(!!enquo(variable)) %>% 
    pull() %>% 
    as.ts() %>% 
    white.test(., lag = 1, qstar = 2, q = 10, range = 4, 
            type = "F", scale = TRUE) %>% 
    tidy()
}

list <- list()
white <- data.frame()

set.seed(123)

for (i in names) {
  list <- white_test(main, all_of(i))
  white <- rbind(white, list)
}

white %<>% 
  cbind(main_longer %>% distinct(Variable)) %>% 
  select(Variable, statistic, p.value) %>% 
  mutate(p.value = round(p.value, 4)) %>% 
  mutate(`No-linealidad` = paste0(round(statistic, 2), " ", 
                             "(", format(p.value, scientific = FALSE), ")")) %>% 
  select(Variable, `No-linealidad`)
```

```{r include=FALSE}
adf_test <- function(data, variable) {
  data %>% 
    select(!!enquo(variable)) %>% 
    pull() %>% 
    as.ts() %>% 
    adf.test(.) %>% 
    tidy()
}

list <- list()
adf <- data.frame()

for (i in names) {
  list <- adf_test(main, all_of(i))
  adf <- rbind(adf, list)
}

adf %<>% 
  cbind(main_longer %>% distinct(Variable)) %>% 
  select(Variable, statistic, p.value) %>% 
  mutate(p.value = round(p.value, 4)) %>% 
  mutate(`Raíz unitaria` = paste0(round(statistic, 2), " ", 
                             "(", format(p.value, scientific = FALSE), ")")) %>% 
  select(Variable, `Raíz unitaria`)
```

```{r include=FALSE}
hurst_exp <- function(data, variable) {
  data %>% 
    select(!!enquo(variable)) %>% 
    pull() %>% 
    as.ts() %>% 
    hurstexp(., display = F) %>% 
    as_tibble() %>% 
    pull(1)
}

list <- list()
hurst <- data.frame()

for (i in names) {
  list <- hurst_exp(main, all_of(i))
  hurst <- rbind(hurst, list)
}

hurst %<>% 
  rename("Hurst" = 1) %>% 
  cbind(main_longer %>% distinct(Variable)) %>% 
  select(Variable, Hurst) %>% 
  mutate(Hurst = round(Hurst, 4))
```

```{r include=FALSE}
table_three <- 
  autocor %>% 
  left_join(jarque) %>% 
  left_join(white) %>% 
  left_join(adf) %>% 
  left_join(hurst) %>% 
  slice(6, 12, 9, 10, 5, 11, 2, 3, 4, 8, 7, 1)
```

\renewcommand{\arraystretch}{1.4}

```{r table_three, echo=FALSE}
kbl(table_three, 
    booktabs = T,
    align = c("crrrrr")) %>%
  kable_styling(font_size = 9,
                full_width = T) %>% 
  row_spec(0, bold = TRUE) %>% 
  group_rows(start_row = 1, 
             end_row = 12,
             latex_gap_space = "-0.5cm") %>% 
  column_spec(1, width = "2cm") %>% 
  column_spec(6, width = "1.5cm")
```

\renewcommand{\arraystretch}{1}

\scriptsize

$^*$ Nivel de significancia entre paréntesis.

\normalsize

\vspace{5mm}

El cuadro 3 muestra un panel de pruebas estadísticas compuesto por un *test* de Ljung--Box para autocorrelación, uno de Jarque--Bera para normalidad, una prueba de red neuronal de White para comprobar si las variables exhiben un comportamiento no-lineal, una prueba de Dickey--Fuller aumentada para verificar si existen raíces unitarias y la estimación del exponente de Hurst para medir la memoria de largo plazo de las series.

Como es de esperar, todas las variables exhiben correlación serial y---tal como evidencia el cuadro de estadísticas descriptivas---la mayor parte de estas no se distribuyen normalmente. La prueba de red neuronal de White confirma que 3 de las series (las tasas de variación del índice mensual de actividad económica, del saldo de la deuda pública externa y de las exportaciones totales) presentan un comportamiento no-lineal.

Además, los resultados de la prueba de raíz unitaria evidencian que las tasas de cambio interanual de la base monetaria restringida, del medio circulante y del índice de precios al consumidor en Estados Unidos no son estacionarias. Por último, el exponente de Hurst demuestra que todas las series exhiben memoria de largo plazo.

## Métodos empíricos

### Modelos de series de tiempo

En la categoría de modelos tradicionales, este estudio considera un modelo de caminata aleatoria estacional (*snaïve*), un modelo de suavizamiento exponencial (*Holt-Winters*), uno autorregresivo (AR), uno autorregresivo integrado de medias móviles estacionales (SARIMA) y un modelo de vectores autorregresivos (VAR). El primer método mencionado, el modelo ingenuo estacional, pronostica los valores futuros de una serie de tiempo asumiendo que cada predicción $h$ es igual al valor observado del mismo mes durante el año anterior, por lo que sus pronósticos tienden a ser relativamente eficientes cuando la variable cuenta con un componente cíclico regular.

Mientras tanto, la metodología detrás de los diferentes sabores de modelos de suavizamiento exponencial consiste en predecir los valores futuros de una serie a través de un promedio ponderado en el que los pesos de los ponderadores decrecen exponencialmente a medida que los valores observados se alejan en el tiempo. El método *Holt-Winters* extiende a este tipo de modelos con el propósito de capturar al componente estacional de la variable que se desea pronosticar. De tal forma, la especificación del modelo comprende tres ecuaciones de suavizamiento: una para dar cuenta del nivel, otra para considerar la tendencia de la serie y la última para modelar su estacionalidad.

Los modelos autorregresivos utilizan una combinación lineal de los valores pasados de la variable objetivo para predecir su comportamiento en el futuro, de modo que una autorregresión de orden $p$ es en esencia una regresión lineal múltiple en la que los predictores son los $p$ valores rezagados de la variable que se desea pronosticar. Un modelo ARIMA combina a este último método con procesos de medias móviles y procedimientos de diferenciación, por lo que es capaz de lidiar con una serie de tiempo no estacionaria al obtener su primera diferencia $d$ veces---hasta inducir consistencia temporal en su media y su varianza---previo a modelarla mediante un proceso ARMA $(p, q)$. Un modelo SARIMA amplía esta metodología para considerar una vasta gama de comportamientos estacionales al incluir términos adicionales que se ajustan a la conducta cíclica de la serie. Estos términos son posteriormente multiplicados por los términos $(p, d, q)$ del modelo ARIMA habitual.

Por último, un modelo de vectores autorregresivos es un método multivariante en el que ninguna relación causal unidireccional entre la variable de interés y sus predictores es impuesta. Mas bien, durante este procedimiento, todas las variables son tratadas simétricamente al ser modeladas como si cada una tuviera influencia sobre el resto de variables incluidas en el sistema: es decir, todas las variables son consideradas endógenas. Un modelo VAR es esencialmente una generalización de un proceso autorregresivo, aplicado a un vector de series de tiempo.

### Modelos de aprendizaje estadístico

Por otro lado, este estudio emplea cinco algoritmos de aprendizaje supervisado. El primero de ellos, la regresión de vectores de soporte (SVR), se basa en un procedimiento en el que un problema de regresión no-lineal es transformado a uno lineal. El algoritmo tiene como propósito encontrar una función $f(x)$ que tenga como máximo una desviación $\epsilon$ de los valores observados $y_i$ y que sea al mismo tiempo lo más plana posible. El problema de regresión se encuentra definido como $f(x) = \langle w \cdot \phi(x) \rangle + b$ donde $\phi(x)$ representa una función no-lineal que proyecta a los datos en un espacio de características en el que es posible encontrar una línea que tenga la mayor distancia vertical entre sus observaciones más cercanas; estos puntos más cercanos al hiperplano son llamados vectores de soporte [@parbat2020].

En este contexto, el concepto de planitud se refiere a que se desea el valor $w$ más pequeño posible. El problema de optimización consiste entonces en encontrar el margen máximo que separa al hiperplano, clasificando correctamente la mayor cantidad de observaciones, por lo que una regresión de vectores de soporte se formula definiendo primero una función de pérdida convexa insensible a ser minimizada y encontrando el tubo más plano que contenga a la mayoría de las instancias de entrenamiento [@smola2004].

Además, son considerados dos métodos que corresponden a la categoría de árboles de decisión, los cuales pueden ser definidos como modelos no-paramétricos basados en la segregación binaria recursiva del espacio de características en un número de regiones más simples. Sin embargo, los árboles de decisión (considerados *weak learners*) son propensos a sobreajustarse a los datos, por lo que una diversidad de métodos de ensamble son empleados para solucionar este problema.

La idea central de estos métodos deviene del hecho de que un gran número de árboles de decisión independientes pueden, conjuntamente, superar el desempeño en las predicciones de uno solo (un fenómeno conocido como *crowd intelligence*).

El primer método basado en árboles de decisión abordado por esta investigación es el algoritmo de bosques aleatorios (RF), un procedimiento que consiste en la construcción de cierta cantidad de árboles de decisión a partir de muestras de entrenamiento generadas a través de un proceso de muestreo aleatorio con reemplazo (*bootstrapping*). Durante el proceso de construcción de los árboles, el algoritmo impide que estos consideren una mayor parte de los predictores disponibles. De esta forma, en promedio, las particiones no toman en cuenta a la variable con mayor capacidad predictiva, por lo que otras variables tienen una mayor oportunidad de ejercer influencia en los modelos. Este proceso de "decorrelación" de los árboles de decisión reduce la varianza en sus resultados, por lo que vuelve a sus predicciones más confiables [@james2013].

El segundo, el algoritmo de potenciación de gradiente extremo (XGB), utiliza un procedimiento de ensamblaje de árboles llamado *boosting*, en el que los árboles de decisión son constituidos de manera secuencial---a diferencia de la construcción simultánea con la que opera el algoritmo de bosques aleatorios---de modo que cada árbol subsecuente aprende de los errores del modelo anterior. Esta metodología incorpora la aplicación de un algoritmo de descenso de gradiente (un proceso de optimización iterativo de primer orden para encontrar un mínimo local de una función objetivo) con el propósito de entrenar al modelo mientras se minimiza su error en la predicción [@pratap2019].

Una red neuronal artificial utiliza como entrada un vector de $p$ variables $X = (X_1, X_2, ...,X_p)$ y con este construye una función no-lineal $f(X)$ que permite predecir a la variable objetivo $Y_i$, de modo que lo que distingue a las diferentes categorías de redes neuronales es la estructura del modelo que se emplea. En este estudio, dos clases de redes neuronales son utilizadas. El perceptrón multicapa (MLP) es considerado la arquitectura de red neuronal más simple cuando únicamente se constituye por tres capas: una capa de entrada, una capa oculta y una capa de salida. Cada capa tiene una cantidad determinada de nodos y, exceptuando a la primera de ellas, cada nodo utiliza una función de activación no-lineal para asignar las entradas ponderadas al valor de salida de cada neurona. A través de un procedimiento llamado propagación hacia atrás (*backpropagation*) los valores de los ponderadores se actualizan minimizando una función de pérdida.

Sin embargo, el perceptrón multicapa asume que los valores de entrada son independientes entre sí. Una clase de redes neuronales más adecuada para datos caracterizados por un fuerte elemento temporal son las redes neuronales recurrentes, diseñadas específicamente para poder predecir datos que son secuenciales por naturaleza. Se dice que las redes neuronales recurrentes tienen memoria ya que estas aceptan información no solo de la entrada actual en una secuencia, sino del estado de la red que surgió al considerar las entradas anteriores en la misma. La arquitectura de larga memoria de corto plazo posee la capacidad de considerar tanto las características contextuales más amplias del modelo (memoria a largo plazo) como la información proporcionada solo por los elementos más recientes de una secuencia (memoria a corto plazo) [@smalterhall2017].

## Validación de modelos y calibración de hiperparámetros

La validación empírica de los modelos es una preocupación constante en la literatura de *machine learning*. Tal y como observan @kuhn2022, debido a que la implementación de estos métodos conlleva una serie de pasos (estimación de parámetros, calibración de hiperparámetros, selección del modelo, evaluación de su desempeño) y, al mismo tiempo, una muestra finita de observaciones, es una práctica común y recomendada dividir al conjunto de datos existente en un subconjunto de observaciones de entrenamiento (el cual es utilizado para optimizar al modelo) y un subconjunto de datos de prueba (que se mantiene en reserva hasta que el método que tiene más probabilidades de éxito sea finalmente escogido).

Sin embargo, el investigador necesita entender qué tan efectivo es su modelo antes de poder emplear el conjunto de observaciones de evaluación. En este sentido, @gareth2013introduction mencionan que los métodos de remuestreo se han convertido en una herramienta esencial durante la aplicación de técnicas modernas de aprendizaje estadístico. Básicamente, estos procedimientos consisten en la extracción repetida de muestras aleatorias al conjunto de datos de entrenamiento con el fin de reajustar al modelo en cada una de ellas para obtener información adicional sobre el modelo en cuestión.

Ahora bien, cuando los datos contienen un fuerte elemento temporal, los métodos usuales de remuestreo tales como validación cruzada y *bootstraping* no son factibles pues el orden de las observaciones tiene que ser preservado para que el modelo pueda aprender los distintos patrones cíclicos y tendenciales que subyacen en la información: una versión más sofisticada de métodos de remuestreo es necesaria.

Este estudio emplea una metodología de remuestreo llamada *rolling forecast origin resampling*, en la que el conjunto de datos de entrenamiento es nuevamente dividido en dos subconjuntos de análisis/evaluación con tamaños específicos, de tal forma que una primera iteración utiliza estos tamaños comenzando desde el inicio de la serie. La segunda iteración utiliza los mismos tamaños pero se desplaza por un número determinado de muestras. El proceso continúa hasta que el conjunto de datos de entrenamiento es agotado por completo, por lo que el origen sobre el cual se basan los pronósticos avanza en el tiempo [@hyndman2018forecasting].

Dicho método de remuestreo es utilizado específicamente con el objetivo de calibrar los hiperparámetros de los modelos de aprendizaje estadístico (en promedio, 3 hiperparámetros diferentes son graduados por modelo). Para esto---y en línea con las metodologías de diseño de experimentos computacionales [@santner2018]---se construye un *space-filling parameter grid* a través de un método llamado *latin hypercube*, que es en esencia un procedimiento estadístico con el que se selecciona, dado un espacio de parámetros, una muestra aleatoria de los mismos.

De tal forma, con 100 combinaciones distintas de hiperparámetros y 6 conjuntos de remuestreo, 600 modelos son estimados durante cada horizonte de pronóstico y aquella combinación de parámetros que minimiza el error absoluto medio (MAE) en los subconjuntos de evaluación es seleccionada para predecir los valores futuros de la variable objetivo en el conjunto de datos de prueba.

## Medidas de precisión

Debido a que existe una amplia gama de medidas de precisión que pueden ser catalogadas en siete grupos distintos [@shcherbakov2013], es importante considerar que la clasificación relativa del resultado de los diversos métodos que se evalúan puede variar según la medida que se utilice [@makridakis2000]. Además, cada medida tiene desventajas que pueden conducir a una evaluación inexacta de los resultados, por lo que---así como @mathews1994 señalan---ninguna medida por sí sola brinda una pauta inequívoca de la eficiencia en las predicciones.

Con el fin de evaluar las diferencias en la precisión de los pronósticos entre métodos tradicionales y de aprendizaje estadístico, este estudio utiliza 3 medidas distintas.

La primera de ellas, el error absoluto medio (MAE), es seleccionada no solo por ser una opción ampliamente utilizada al comparar distintos métodos que predicen un solo conjunto de datos, sino por contar con la ventaja de presentar una mayor robustez a valores atípicos que otras medidas dependientes de escala como la raíz del error cuadrático medio (RMSE) [@shcherbakov2013]. Esta medida se define como,

\vspace{-2mm}

$$
MAE = \frac{1}{n}\sum_{i=1}^{n}|y_{t}-\hat{y}_{t}|.
$$

La segunda medida a considerar será el error absoluto medio relativo (rMAE), que es calculado como el ratio entre el MAE del método $i$ que se desea evaluar y el $MAE_{rw}$ de un modelo de caminata aleatoria, utilizado como punto de referencia. @hyndman2006 observan que una de las ventajas de esta medida es su interpretabilidad: Un $rMAE > 1$ significa que el método propuesto se desempeña peor que lo que lo hace el modelo de referencia, mientras que un $rMAE < 1$ denota lo opuesto. Formalmente,

\vspace{-4mm}

$$rMAE = \frac{MAE_i}{MAE_{rw}}.
$$

La última medida que será considerada en la presente investigación es el error medio absoluto escalado (MASE), un método propuesto por primera vez en @hyndman2006 con el objeto de brindar una mejor opción que el error porcentual absoluto medio simétrico (sMAPE). El error medio absoluto escalado se encuentra definido de la siguiente manera:

\vspace{-2.5mm}

$$MASE=\frac{1}{k}\frac{\sum_{t=1}^{k}|y_{t}-\hat{y}_{t}|}{\frac{1}{n-m}\sum_{t=1}^{k}|y_{t}-\hat{y}_{t-m}|},
$$

donde $n$ es el número de observaciones históricas disponibles y $m$ es la frecuencia de la serie de tiempo. Su interpretación es similar a la del rMAE: un $MASE < 1$ indica que los pronósticos del modelo evaluado son, en promedio, más exactos que los pronósticos de un modelo de referencia [@makridakis2018].

En su implementación original, esta medida utiliza el error absoluto medio de un modelo de caminata aleatoria dentro de la muestra para calcular los errores escalados. @Kuhn2021 observan que el procedimiento es llevado a cabo de tal modo (en lugar de considerar el error fuera de muestra) ya que existe la posibilidad de que el error fuera de muestra no pueda ser calculado al pronosticar un periodo hacia delante. No obstante, la implementación del MASE en R solo conoce los valores genuinos y estimados fuera de muestra, por lo que es este último error el que se utiliza durante el cómputo de dicha medida de precisión.

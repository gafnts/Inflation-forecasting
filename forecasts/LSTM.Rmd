# Long short-term memory

```{r}
pacman::p_load(tidyverse, tidymodels, timetk, tibbletime, here, keras, tictoc)
```

## (b) Model construction

```{r}
data <- read_csv(here("data", "main.csv")) %>% rename("index" = fecha)
```

### Train and test sets

```{r}
splits <- data %>% time_series_split(index, assess = "24 months", cumulative = TRUE)
```

### Model validation

```{r}
resamples <- 
  rolling_origin(
    data,
    initial = 12 * 10,
    assess = 12 * 2,
    cumulative = FALSE,
    skip = 12
)
```

### Single LSTM

```{r}
split <- resamples$splits[[5]]
split_id <- resamples$id[[5]]
```

### Data setup

```{r}
train <- training(split)
test <- testing(split)

df <- bind_rows(
    train %>% add_column(key = "training"),
    test %>% add_column(key = "testing")
) %>% 
    as_tbl_time(index = index)

df
```

### Feature engineering

```{r}
recipe <- 
  recipe(ipc ~ ., df) %>% 
  step_center(ipc, all_numeric_predictors()) %>%
  step_scale(ipc, all_numeric_predictors()) %>%
  #step_lag(all_numeric_predictors(), -ipc, lag = 24) %>% 
  prep()

df_processed <- bake(recipe, df)

center <- recipe$steps[[1]]$means["ipc"]
scale  <- recipe$steps[[2]]$sds["ipc"]
```

### Model inputs

```{r}
lag_setting <- 24
batch_size <- 12
train_length <- 120
tsteps <- 1
epochs <- 50
```

### 2D and 3D train/test array

```{r}
# Training set
lag_train_tbl <- 
  df_processed %>%
  filter(key == "training") %>%
  tail(train_length)

x_train_mat <- as.matrix(lag_train_tbl %>% select(-key, -ipc, -index))
x_train_arr <- array(data = x_train_mat, dim = c(length(x_train_mat), 1, 13))

y_train_vec <- lag_train_tbl$ipc
y_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))

# Testing set
lag_test_tbl <- 
  df_processed %>%
  filter(key == "testing") %>%
  tail(train_length)

x_test_mat <- as.matrix(lag_test_tbl %>% select(-key, -ipc, -index))
x_test_arr <- array(data = x_test_mat, dim = c(length(x_test_mat), 1, 13))

y_test_vec <- lag_test_tbl$ipc
y_test_arr <- array(data = y_test_vec, dim = c(length(y_test_vec), 1))
```

### Building the model

```{r}
model <- keras_model_sequential()

model %>% 
  layer_lstm(units = 35, 
             input_shape = c(tsteps, 1), 
             batch_size = batch_size,
             return_sequences = TRUE, 
             stateful = TRUE) %>% 
  layer_lstm(units = 35, 
             return_sequences = FALSE, 
             stateful = TRUE) %>% 
    layer_dense(units = 1)

model %>% 
    compile(loss = 'mae', optimizer = 'adam')

model
```

### Fitting the model

```{r}
for (i in 1:epochs) {
    model %>% fit(x = x_train_arr, 
                  y = y_train_arr, 
                  batch_size = batch_size,
                  epochs = 1, 
                  verbose = 1, 
                  shuffle = FALSE)
    
    model %>% reset_states()
    cat("Epoch: ", i)
}
```

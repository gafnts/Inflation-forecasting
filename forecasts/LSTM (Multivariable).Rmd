# Long short-term memory

```{r}
pacman::p_load(tidyverse, tidymodels, magrittr, timetk, tibbletime, here, keras, tictoc)
```

```{r}
data <- 
  read_csv(here("data", "main.csv")) %>% 
  rename("date" = fecha)

date <- data %>% select(date)
```

```{r}
recipe <- 
  recipe(ipc ~ ., data) %>%
  step_rm(date) %>% 
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  prep()

baked <- bake(recipe, data)
baked

center <- recipe$steps[[2]]$means["value"]
scale  <- recipe$steps[[3]]$sds["value"]
```

```{r}
splits <- baked %>% 
  bind_cols(date) %>% 
  relocate(date) %>% 
  time_series_split(date, assess = "24 months", cumulative = TRUE)

train <- training(splits)
test <- testing(splits)
```

```{r}
train_mat <- train %>% select(-date) %>% as.matrix()
test_mat <- test %>% select(-date) %>% as.matrix()
```

```{r}
lag_setting <- 35
batch_size <- 24
train_length <- 120
tsteps <- 1
epochs <- 300
```

```{r}
model <- keras_model_sequential()

model %>%
  layer_lstm(units = 50, 
             input_shape = c(tsteps, 1), 
             batch_size = batch_size,
             return_sequences = TRUE,
             stateful = TRUE) %>% 
  layer_gru(units = 50, 
             input_shape = c(tsteps, 1), 
             batch_size = batch_size,
             return_sequences = TRUE,
             stateful = TRUE) %>% 
  layer_lstm(units = 50, 
             return_sequences = FALSE,
             stateful = TRUE) %>% 
  layer_dense(units = 1)

model %>% compile(loss = 'mae', optimizer = 'adam')

model
```

```{r}
model %>% 
  fit(x = x_train_arr, 
      y = y_train_arr, 
      batch_size = batch_size,
      steps_per_epochs = 300,
      epochs = 300, 
      verbose = 1, 
      shuffle = FALSE)
```

```{r}
pacman::p_load(tidyverse, here, magrittr, janitor, readxl)
```

# Direct forecasting

## a. Inflation: Secmca

```{r}
secmca <- 
  read_csv("secmca.csv") %>% 
  slice(4:390)

secmca
```

```{r}
tidy_secmca <- 
  secmca %>% 
  select(1, 10:13) %>% 
  row_to_names(2) %>% 
  clean_names() %>% 
  rename(fecha = "na")  %>% 
  mutate_at(vars(c(2:5)), list(gsub), pattern = "--", replacement = "0") %>% 
  mutate_at(vars(c(2:5)), list(as.double)) %>% 
  mutate(pais = "Guatemala") %>% 
  rename(ipc = "ipc_general",
         var_mensual = "ipc_variacion_mensual",
         var_interanual = "ipc_variacion_interanual",
         var_acum = "ipc_variacion_acumulada_con_diciembre_anterior") %>% 
  select(fecha, pais, everything())
```

```{r}
inf <- 
  tidy_secmca %>% 
  slice(121:384) %>% 
  select(var_interanual) %>% 
  rename(Guatemala = "var_interanual")

inf
```

## b. Consumer price index: INE

```{r}

```

## c. Multi-output forecasting

```{r}
pacman::p_load(tidymodels, forecastML, glmnet, randomForest)
```

The multi-output forecasting approach used in `forecastML` involves the following steps:

1.  Build a single multi-output model that simultaneously forecasts over both short- and long-term forecast horizons.

2.  Assess model generalization performance across a variety of heldout datasets through time.

3.  Select the hyperparameters that minimize forecast error over the relevant forecast horizons and re-train.

```{r}
date_frequency <- "1 month"
dates <- seq(as.Date("2000-01-01"), as.Date("2021-12-01"), by = date_frequency)
```

```{r}
bind_cols(inf, dates) %>% tail() #just checking
```

```{r}
inf_train <- inf[1:(nrow(inf) - 55), ]
inf_test <- inf[(nrow(inf) - 55 + 1):nrow(inf), ]
```

```{r}
inf %>% 
  ggplot(aes(x = dates, y = Guatemala)) + 
  geom_line() +
  geom_vline(xintercept = dates[nrow(inf_train)], color = "red", size = 1.1) +
  theme_bw() + xlab("") + ylab("")
```

### Lagged dataframe

```{r}
outcome_col <- 1
horizons <- c(1, 3, 6, 12)
lookback <- c(1:6, 9, 12, 15)

inf_list <- create_lagged_df(inf_train,
                             outcome_col = outcome_col,
                             type = "train",
                             horizons = horizons,
                             lookback = lookback,
                             date = dates[1:nrow(inf_train)],
                             frequency = date_frequency)
```

```{r}
plot(inf_list)
```

### Windows

```{r}
windows <- create_windows(lagged_df = inf_list, 
                          window_length = 12, 
                          skip = 48,
                          window_start = NULL, 
                          window_stop = NULL,
                          include_partial_window = TRUE)

windows
```

```{r}
plot(windows, inf_list, show_labels = TRUE)
```

### Model training

```{r}
# Lasso
model_function <- function(data) {
    
  constant_features <- 
    which(unlist(lapply(data[, -1], function(x) {!(length(unique(x)) > 1)})))
  
  if (length(constant_features) > 1) {
    data <- data[, -c(constant_features + 1)]
  }

  x <- data[, -(1), drop = FALSE]
  y <- data[, 1, drop = FALSE]
  x <- as.matrix(x, ncol = ncol(x))
  y <- as.matrix(y, ncol = ncol(y))

  model <- glmnet::cv.glmnet(x, y, nfolds = 3)
  return(list("model" = model, "constant_features" = constant_features))
}

# Random Forest
model_function_2 <- function(data) {

  outcome_names <- names(data)[1]
  model_formula <- formula(paste0(outcome_names,  "~ ."))

  model <- randomForest::randomForest(formula = model_formula, data = data, ntree = 200)
  return(model)
}
```

# **Multilayer perceptron**

```{r}
pacman::p_load(tidyverse, tidymodels, timetk, here, keras, tictoc, vip)
data <- read_csv(here("data", "main.csv")) %>% rename("date" = fecha)
```

## (a) Multiple forecasting horizons

### Forecast function

```{r}
perceptron <-
  function(assess) {
    tic("MLP")
    
    # Training and testing partitions
    splits <- data %>% time_series_split(date, assess = assess, cumulative = TRUE)
    train <- training(splits)
    
    # Model specification
    mlp <-
      mlp(
        hidden_units = tune(),
        dropout = tune(),
        epochs = 200,
        activation = "relu"
      ) %>%
      set_engine("keras") %>%
      set_mode("regression")
    
    # Feature engineering
    recipe <- train %>% recipe(ipc ~ .) %>% step_rm(date)
    
    # Workflow
    workflow <- workflow() %>% add_model(mlp) %>% add_recipe(recipe)
    
    # Hyperparameter tuning
    resamples <-
      train %>%
      rolling_origin(
        initial = (10 * 12),
        assess = (2 * 12),
        skip = 16,
        cumulative = TRUE
      )
    
    grid <- 
      grid_latin_hypercube(
        hidden_units(),
        dropout(),
        size = 15)
    
    metrics <- metric_set(mae, mase)
    
    doParallel::registerDoParallel()
    
    set.seed(123)
    tune <- 
      tune_grid(
        workflow,
        resamples = resamples,
        grid = grid,
        metrics = metrics,
        control = control_grid(save_pred = TRUE)
      )
    
    best_mae <- select_best(tune, "mae")
    
    # Final workflow
    final_workflow <-
      finalize_workflow(workflow,
                        best_mae)
    
    # Final model
    set.seed(234)
    model <- last_fit(final_workflow, splits, metrics = metrics)
    metrics <- collect_metrics(model)
    
    # Forecast
    forecast <- model %>% collect_predictions() %>% select(.pred, .row, ipc)
    
    # Time elapsed
    toc(log = TRUE)
    time <- tic.log(format = TRUE)
    tic.clearlog()
    
    # Collect results
    results <- list(forecast, metrics, time)
    return(results)
  }
```

### Forecast horizons

```{r}
mlp_3 <- perceptron("3 months")

mlp_3[[1]] %>% write_csv(here("results", "mlp_3_forecast.csv"))
mlp_3[[2]] %>% write_csv(here("results", "mlp_3_metrics.csv"))
mlp_3[[3]][[1]] %>% as_tibble() %>% write_csv(here("results", "mlp_3_time.csv"))
```

```{r}
mlp_9 <- perceptron("9 months")

mlp_9[[1]] %>% write_csv(here("results", "mlp_9_forecast.csv"))
mlp_9[[2]] %>% write_csv(here("results", "mlp_9_metrics.csv"))
mlp_9[[3]][[1]] %>% as_tibble() %>% write_csv(here("results", "mlp_9_time.csv"))
```

```{r}
mlp_12 <- perceptron("12 months")

mlp_12[[1]] %>% write_csv(here("results", "mlp_12_forecast.csv"))
mlp_12[[2]] %>% write_csv(here("results", "mlp_12_metrics.csv"))
mlp_12[[3]][[1]] %>% as_tibble() %>% write_csv(here("results", "mlp_12_time.csv"))
```

```{r}
mlp_24 <- perceptron("24 months")

mlp_24[[1]] %>% write_csv(here("results", "mlp_24_forecast.csv"))
mlp_24[[2]] %>% write_csv(here("results", "mlp_24_metrics.csv"))
mlp_24[[3]][[1]] %>% as_tibble() %>% write_csv(here("results", "mlp_24_time.csv"))
```

### Safety checks

```{r}
mlp_24[[1]]%>% 
  pivot_longer(-.row) %>% 
  ggplot(aes(.row, value, color = name)) +
  geom_line()
```

## (b) Model construction

### Train and test sets

```{r}
splits <- data %>% time_series_split(date, assess = "24 months", cumulative = TRUE)
train <- training(splits)
```

### Model specification

```{r}
mlp <-
  mlp(hidden_units = tune(),
      dropout = tune(),
      epochs = 200,
      activation = "relu") %>%
  set_engine("keras") %>% 
  set_mode("regression")
```

### Feature engineering

```{r}
recipe <- 
  train %>% 
  recipe(ipc ~ .) %>% 
  step_rm(date)
```

```{r}
recipe %>% prep() %>% juice()
```

### Workflow

```{r}
workflow <- 
  workflow() %>% 
  add_model(mlp) %>% 
  add_recipe(recipe)
```

### Hyperparameter tuning

```{r}
resamples <- 
  train %>% 
  rolling_origin(
    initial = 10 * 12,
    assess = 2 * 12,
    skip = 16,
    cumulative = TRUE)
```

```{r}
resamples %>%
  tk_time_series_cv_plan() %>%
  plot_time_series_cv_plan(date, ipc, 
                           .facet_ncol = 2, .interactive = F)
```

```{r}
grid <- 
  grid_latin_hypercube(
    hidden_units(),
    dropout(),
    size = 15
)

metrics <- metric_set(mae, mase)
```

```{r}
tic("MLP")

set.seed(123)
tune <- tune_grid(
  workflow,
  resamples = resamples,
  grid = grid,
  metrics = metrics,
  control = control_grid(save_pred = TRUE)
)

toc(log = TRUE)
time <- tic.log(format = TRUE)
tic.clearlog()
```

```{r}
tune %>% collect_metrics() %>% print(n = Inf)
best_mae <- select_best(tune, "mae"); best_mae
```

```{r}
tune %>%
  collect_metrics() %>%
  filter(.metric == "mae") %>%
  select(mean, hidden_units:dropout) %>%
  pivot_longer(hidden_units:dropout,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "MAE")
```

### Final model

```{r}
final <- 
  finalize_workflow(
  workflow,
  best_mae
)
```

```{r}
final_res <- last_fit(final, splits, metrics = metrics)
collect_metrics(final_res)
```

### Forecast

```{r}
final_res %>%
  collect_predictions() %>% 
  select(.pred, .row, ipc) %>% 
  pivot_longer(-.row) %>% 
  ggplot(aes(.row, value, color = name)) +
  geom_line()
```

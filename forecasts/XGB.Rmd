# XGB

```{r}
pacman::p_load(tidyverse, tidymodels, timetk, here, xgboost, tictoc, vip)
data <- read_csv(here("data", "main.csv")) %>% rename("date" = fecha)
```

## (a) Forecast function

```{r}
xg_boost <-
  function(assess) {
    tic("XGBoost")
    
    # Training and testing partitions
    splits <- data %>% time_series_split(date, assess = assess, cumulative = TRUE)
    train <- training(splits)
    
    # Model specification
    xgb <-
      boost_tree(
        trees = 1000,
        tree_depth = tune(),
        min_n = tune(),
        loss_reduction = tune(),
        sample_size = tune(),
        mtry = tune(),
        learn_rate = tune(),
      ) %>%
      set_engine("xgboost") %>%
      set_mode("regression")
    
    # Feature engineering
    recipe <- train %>% recipe(ipc ~ .) %>% step_rm(date)
    
    # Workflow
    workflow <- workflow() %>% add_model(xgb) %>% add_recipe(recipe)
    
    # Hyperparameter tuning
    resamples <-
      train %>%
      rolling_origin(
        initial = (8 * 12),
        assess = (2 * 12),
        skip = 10,
        cumulative = TRUE
      )
    
    grid <-
      grid_latin_hypercube(
        tree_depth(),
        min_n(),
        loss_reduction(),
        sample_size = sample_prop(),
        finalize(mtry(), train),
        learn_rate(),
        size = 100
      )
    
    metrics <- metric_set(mae, mase)
    
    doParallel::registerDoParallel()
    
    set.seed(123)
    tune <- 
      tune_grid(
        workflow,
        resamples = resamples,
        grid = grid,
        metrics = metrics,
        control = control_grid(save_pred = TRUE)
      )
    
    best_mae <- select_best(tune, "mae")
    
    # Final workflow
    final_workflow <-
      finalize_workflow(workflow,
                        best_mae)
    
    # Final model
    model <- last_fit(final_workflow, splits, metrics = metrics)
    metrics <- collect_metrics(model)
    
    # Forecast
    forecast <- model %>% collect_predictions() %>% select(.pred, .row, ipc)
    
    # Time elapsed
    toc(log = TRUE)
    time <- tic.log(format = TRUE)
    tic.clearlog()
    
    # Collect results
    results <- list(forecast, metrics, time)
    return(results)
  }
```

```{r}
xgb_3 <- xg_boost("3 months")

xgb_3[[1]] %>% write_csv(here("results", "xgb_3_forecast.csv"))
xgb_3[[2]] %>% write_csv(here("results", "xgb_3_metrics.csv"))
xgb_3[[3]][[1]] %>% as_tibble() %>% write_csv(here("results", "xgb_3_time.csv"))
```

```{r}
xgb_9 <- xg_boost("9 months")

xgb_9[[1]] %>% write_csv(here("results", "xgb_9_forecast.csv"))
xgb_9[[2]] %>% write_csv(here("results", "xgb_9_metrics.csv"))
xgb_9[[3]][[1]] %>% as_tibble() %>% write_csv(here("results", "xgb_9_time.csv"))
```

```{r}
xgb_12 <- xg_boost("12 months")

xgb_12[[1]] %>% write_csv(here("results", "xgb_12_forecast.csv"))
xgb_12[[2]] %>% write_csv(here("results", "xgb_12_metrics.csv"))
xgb_12[[3]][[1]] %>% as_tibble() %>% write_csv(here("results", "xgb_12_time.csv"))
```

```{r}
xgb_24 <- xg_boost("24 months")

xgb_24[[1]] %>% write_csv(here("results", "xgb_24_forecast.csv"))
xgb_24[[2]] %>% write_csv(here("results", "xgb_24_metrics.csv"))
xgb_24[[3]][[1]] %>% as_tibble() %>% write_csv(here("results", "xgb_24_time.csv"))
```

### Safety checks

```{r}
xgb_24[[1]] %>% 
  pivot_longer(-.row) %>% 
  ggplot(aes(.row, value, color = name)) +
  geom_line()
```

## (b) Model building

### Train and test sets

```{r}
splits <- data %>% time_series_split(date, assess = "24 months", cumulative = TRUE)
train <- training(splits)
```

### Model specification

```{r}
xgb <- 
  boost_tree(
      trees = 1000, 
      tree_depth = tune(), min_n = tune(), 
      loss_reduction = tune(),
      sample_size = tune(), mtry = tune(),
      learn_rate = tune(),   
    ) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")
```

### Feature engineering

```{r}
recipe <- 
  train %>% 
  recipe(ipc ~ .) %>% 
  step_rm(date)
```

```{r}
recipe %>% prep() %>% juice()
```

### Workflow

```{r}
workflow <- 
  workflow() %>% 
  add_model(xgb) %>% 
  add_recipe(recipe)
```

### Hyperparameter tuning

```{r}
resamples <- 
  train %>% 
  rolling_origin(
    initial = 8 * 12,
    assess = 2 * 12,
    skip = 10,
    cumulative = TRUE)
```

```{r}
resamples %>%
  tk_time_series_cv_plan() %>%
  plot_time_series_cv_plan(date, ipc, 
                           .facet_ncol = 2, .interactive = F)
```

```{r}
grid <- 
  grid_latin_hypercube(
    tree_depth(),
    min_n(),
    loss_reduction(),
    sample_size = sample_prop(),
    finalize(mtry(), train),
    learn_rate(),
    size = 15
)
```

```{r}
doParallel::registerDoParallel()

metrics <- metric_set(mae, mase)

tic("XGBoost")

set.seed(123)
tune <- tune_grid(
  workflow,
  resamples = resamples,
  grid = grid,
  metrics = metrics,
  control = control_grid(save_pred = TRUE)
)

toc(log = TRUE)
time <- tic.log(format = TRUE)
tic.clearlog()
```

```{r}
tune %>% collect_metrics() %>% print(n = Inf)
best_mae <- select_best(tune, "mae"); best_mae
```

```{r}
tune %>%
  collect_metrics() %>%
  filter(.metric == "mae") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "MAE")
```

### Final model

```{r}
final <- 
  finalize_workflow(
  workflow,
  best_mae
)
```

```{r}
final %>%
  fit(data = train) %>%
  extract_fit_parsnip() %>%
  vip(geom = "col")
```

```{r}
final_res <- last_fit(final, splits, metrics = metrics)
collect_metrics(final_res)
```

### Forecast

```{r}
final_res %>%
  collect_predictions() %>% 
  select(.pred, .row, ipc) %>% 
  pivot_longer(-.row) %>% 
  ggplot(aes(.row, value, color = name)) +
  geom_line()
```
